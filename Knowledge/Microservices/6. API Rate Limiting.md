# Introduction

**Rate limiting** is essential component in API Gateway that restricts how many requests a client can send to an API within a defined time frame. Its primary goal is to **protect backend services** from overload, abuse while ensuring fair usage across clients.

The API Gateway checks rate limits before forwarding requests to backend microservices. Each incoming request is evaluated against configured limits. If the request count exceeds the allowed threshold, the gateway rejects further requests with an HTTP **429 (Too Many Requests)** response, often including headers that indicate when the client can retry.

**In short:** API rate limiting is the API’s way of saying: 

> _“You can only call me this many times per time window — don’t overload me.”_

Rate limiting is typically implemented using algorithms such as :

1. **Fixed Window**
2. **Sliding Window**
3. **Token Bucket** 
4. **Leaky Bucket**

**Fixed Window rate limiting** works by dividing time into fixed intervals such as one minute or one hour and counting how many requests a client sends during that interval. Once the limit is reached, any additional requests are blocked until the next time window starts, when the counter resets. While this approach is easy to implement, it can cause sudden traffic spikes at the boundary of two windows, because a client may send many requests at the end of one window and again at the start of the next.

**Sliding Window rate limiting** improves on the fixed window approach by always considering requests made during the most recent time period instead of fixed blocks. Rather than resetting counters at exact time boundaries, the system continuously checks how many requests were made in the last few seconds or minutes. This results in smoother traffic control and avoids sudden spikes, but it requires more memory and processing because each request must be tracked with a timestamp.
*Store timestamps of each request in a log. When a new request arrives, count all requests within last X seconds (sliding time). If the count exceeds the limit → block.*

**Token Bucket rate limiting** works by giving each client a bucket that holds tokens. Tokens are added to the bucket at a fixed rate over time, and every request consumes one token. If tokens are available, the request is allowed; if not, it is rejected. When traffic is low, tokens accumulate, allowing short bursts of requests later. This makes the token bucket algorithm very popular in API Gateways because it balances traffic control while still supporting occasional bursts.

**Leaky Bucket rate limiting** controls traffic by processing requests at a constant rate, similar to water leaking from a bucket drop by drop. Incoming requests are placed into a queue and are processed steadily. If requests arrive too quickly and the bucket becomes full, additional requests are dropped. This ensures a smooth and predictable request flow to backend services, but it does not handle sudden bursts well and can introduce delays.

In practice, **API Gateways commonly use the Token Bucket algorithm** because it provides a good balance between protecting backend services and offering flexibility to clients.
